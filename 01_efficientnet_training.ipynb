{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet model training\n\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-24T11:35:10.684779Z"
    }
   },
   "source": [
    "# EfficientNet model training\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ EfficientNet-B3 Training Pipeline\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Device: {device}\")\n",
    "\n",
    "# Load configuration\n",
    "processed_root = Path(\"../../data/processed\")\n",
    "results_dir = Path(\"../../results\")\n",
    "\n",
    "with open(processed_root / \"training_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "class_weights = torch.load(processed_root / \"fast_class_weights.pt\", map_location='cpu')\n",
    "\n",
    "print(f\"üìã Training Configuration:\")\n",
    "print(f\"   Classes: {config['num_classes']}\")\n",
    "print(f\"   Class names: {config['class_names']}\")\n",
    "print(f\"   Image size: {config['image_size']}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Recreate the fast dataset (we need to reload it)\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "# Load simplified mappings\n",
    "with open(results_dir / \"simplified_class_mapping.json\", \"r\") as f:\n",
    "    class_mapping = json.load(f)\n",
    "\n",
    "with open(results_dir / \"detailed_to_simplified_mapping.json\", \"r\") as f:\n",
    "    detailed_to_simplified = json.load(f)\n",
    "\n",
    "class FastVehicleDataset:\n",
    "    \"\"\"Lightweight dataset for training\"\"\"\n",
    "    \n",
    "    def __init__(self, split_name, max_files=100, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data(split_name, max_files)\n",
    "        print(f\"üì¶ {split_name} dataset: {len(self.data)} samples\")\n",
    "    \n",
    "    def _load_data(self, split_name, max_files):\n",
    "        data_root = Path(\"../../data/raw\")\n",
    "        annos_dir = data_root / split_name / \"annos\"\n",
    "        images_dir = data_root / split_name / \"images\"\n",
    "        \n",
    "        xml_files = list(annos_dir.glob(\"*.xml\"))[:max_files]\n",
    "        data_samples = []\n",
    "        \n",
    "        for xml_file in xml_files:\n",
    "            try:\n",
    "                tree = ET.parse(xml_file)\n",
    "                root = tree.getroot()\n",
    "                \n",
    "                img_id = xml_file.stem\n",
    "                img_path = images_dir / f\"{img_id}.jpg\"\n",
    "                \n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "                \n",
    "                for obj in root.findall('object'):\n",
    "                    try:\n",
    "                        name_elem = obj.find('name')\n",
    "                        if name_elem is None:\n",
    "                            continue\n",
    "                        \n",
    "                        detailed_class = name_elem.text.strip().lower()\n",
    "                        simplified_class = detailed_to_simplified.get(detailed_class, 'unknown')\n",
    "                        \n",
    "                        if simplified_class == 'unknown':\n",
    "                            continue\n",
    "                        \n",
    "                        bbox_elem = obj.find('bndbox')\n",
    "                        if bbox_elem is None:\n",
    "                            continue\n",
    "                        \n",
    "                        xmin = max(0, int(float(bbox_elem.find('xmin').text)))\n",
    "                        ymin = max(0, int(float(bbox_elem.find('ymin').text)))\n",
    "                        xmax = int(float(bbox_elem.find('xmax').text))\n",
    "                        ymax = int(float(bbox_elem.find('ymax').text))\n",
    "                        \n",
    "                        if xmax > xmin + 30 and ymax > ymin + 30:\n",
    "                            data_samples.append({\n",
    "                                'img_path': str(img_path),\n",
    "                                'bbox': [xmin, ymin, xmax, ymax],\n",
    "                                'class_idx': class_mapping[simplified_class],\n",
    "                                'class_name': simplified_class\n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return data_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            item = self.data[idx]\n",
    "            image = Image.open(item['img_path']).convert('RGB')\n",
    "            bbox = item['bbox']\n",
    "            cropped = image.crop(bbox)\n",
    "            \n",
    "            if self.transform:\n",
    "                cropped = self.transform(cropped)\n",
    "            \n",
    "            return cropped, item['class_idx']\n",
    "        except:\n",
    "            # Return dummy data if error\n",
    "            dummy_img = torch.zeros(3, 224, 224)\n",
    "            return dummy_img, 0\n",
    "\n",
    "# Define transforms for training\n",
    "def get_training_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomCrop((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def get_val_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# Create datasets\n",
    "print(f\"\\nüìä CREATING TRAINING DATASETS\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "train_dataset = FastVehicleDataset('train', max_files=100, transform=get_training_transforms())\n",
    "val_dataset = FastVehicleDataset('val', max_files=50, transform=get_val_transforms())\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"üîÑ Data loaders ready:\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Create EfficientNet model\n",
    "print(f\"\\nüèóÔ∏è CREATING EFFICIENTNET-B3 MODEL\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "class EfficientNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=6, pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load pretrained EfficientNet-B3\n",
    "        self.backbone = timm.create_model('efficientnet_b3', pretrained=pretrained)\n",
    "        \n",
    "        # Get number of features\n",
    "        num_features = self.backbone.classifier.in_features\n",
    "        \n",
    "        # Replace classifier\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Initialize model\n",
    "model = EfficientNetClassifier(num_classes=config['num_classes'])\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ EfficientNet-B3 model created\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Create the missing directories first\n",
    "models_dir = Path(\"../../models/saved_models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results_plots_dir = Path(\"../../results/plots\")\n",
    "results_plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Created directories:\")\n",
    "print(f\"   {models_dir}\")\n",
    "print(f\"   {results_plots_dir}\")\n",
    "\n",
    "# Setup training\n",
    "print(f\"\\n‚öôÔ∏è TRAINING SETUP\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "print(f\"‚úÖ Training setup complete:\")\n",
    "print(f\"   Loss: CrossEntropyLoss with class weights\")\n",
    "print(f\"   Optimizer: AdamW (lr=0.001)\")\n",
    "print(f\"   Scheduler: CosineAnnealingLR\")\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with tqdm(train_loader, desc=\"Training\", leave=False) as pbar:\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.3f}',\n",
    "                'Acc': f'{100.*correct/total:.1f}%'\n",
    "            })\n",
    "    \n",
    "    return running_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(val_loader), 100. * correct / total\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\nüèÉ STARTING TRAINING\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "num_epochs = 10  # Start with fewer epochs for quick testing\n",
    "best_acc = 0.0\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Save best model with proper path\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        model_save_path = models_dir / \"efficientnet_best.pth\"\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"‚úÖ New best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüèÅ TRAINING COMPLETE!\")\n",
    "print(\"=\" * 25)\n",
    "print(f\"‚è±Ô∏è Training time: {training_time:.2f}s\")\n",
    "print(f\"üéØ Best validation accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "plt.plot(val_losses, label='Val Loss', color='red')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Acc', color='blue')\n",
    "plt.plot(val_accs, label='Val Acc', color='red')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_plots_dir / \"efficientnet_training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save training results\n",
    "training_results = {\n",
    "    'model': 'EfficientNet-B3',\n",
    "    'best_val_accuracy': float(best_acc),\n",
    "    'final_train_accuracy': float(train_accs[-1]),\n",
    "    'final_val_accuracy': float(val_accs[-1]),\n",
    "    'training_time': float(training_time),\n",
    "    'epochs': num_epochs,\n",
    "    'train_losses': train_losses,\n",
    "    'train_accs': train_accs,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accs': val_accs,\n",
    "    'parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'model_size_mb': sum(p.numel() * 4 for p in model.parameters()) / (1024 * 1024)  # Approximate MB\n",
    "}\n",
    "\n",
    "with open(results_dir / \"efficientnet_training_results.json\", \"w\") as f:\n",
    "    json.dump(training_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ RESULTS SAVED:\")\n",
    "print(f\"   Model weights: {model_save_path}\")\n",
    "print(f\"   Training curves: {results_plots_dir}/efficientnet_training_curves.png\")\n",
    "print(f\"   Results: {results_dir}/efficientnet_training_results.json\")\n",
    "\n",
    "print(f\"\\nüìä FINAL RESULTS:\")\n",
    "print(f\"   üéØ Best Val Accuracy: {best_acc:.2f}%\")\n",
    "print(f\"   üìà Final Train Accuracy: {train_accs[-1]:.2f}%\")\n",
    "print(f\"   ‚è±Ô∏è Training Time: {training_time:.1f}s\")\n",
    "print(f\"   üèãÔ∏è Model Size: {training_results['model_size_mb']:.1f}MB\")\n",
    "\n",
    "print(f\"\\nüöÄ EFFICIENTNET TRAINING COMPLETE!\")\n",
    "print(\"Next: Train ResNet with Attention mechanism\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ EfficientNet-B3 Training Pipeline\n",
      "========================================\n",
      "üîß Device: cpu\n",
      "üìã Training Configuration:\n",
      "   Classes: 6\n",
      "   Class names: ['auto_rickshaw', 'bus', 'car', 'motorcycle', 'scooter', 'truck']\n",
      "   Image size: 224\n",
      "   Device: cpu\n",
      "\n",
      "üìä CREATING TRAINING DATASETS\n",
      "================================\n",
      "üì¶ train dataset: 414 samples\n",
      "üì¶ val dataset: 203 samples\n",
      "üîÑ Data loaders ready:\n",
      "   Train batches: 52\n",
      "   Val batches: 26\n",
      "\n",
      "üèóÔ∏è CREATING EFFICIENTNET-B3 MODEL\n",
      "===================================\n",
      "‚úÖ EfficientNet-B3 model created\n",
      "   Parameters: 11,091,246\n",
      "   Trainable: 11,091,246\n",
      "‚úÖ Created directories:\n",
      "   ..\\..\\models\\saved_models\n",
      "   ..\\..\\results\\plots\n",
      "\n",
      "‚öôÔ∏è TRAINING SETUP\n",
      "====================\n",
      "‚úÖ Training setup complete:\n",
      "   Loss: CrossEntropyLoss with class weights\n",
      "   Optimizer: AdamW (lr=0.001)\n",
      "   Scheduler: CosineAnnealingLR\n",
      "\n",
      "üèÉ STARTING TRAINING\n",
      "=========================\n",
      "\n",
      "Epoch 1/10\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2396, Train Acc: 55.80%\n",
      "Val Loss: 1.3076, Val Acc: 73.40%\n",
      "LR: 0.000976\n",
      "‚úÖ New best model saved! (Val Acc: 73.40%)\n",
      "\n",
      "Epoch 2/10\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9984, Train Acc: 69.57%\n",
      "Val Loss: 0.9580, Val Acc: 75.37%\n",
      "LR: 0.000905\n",
      "‚úÖ New best model saved! (Val Acc: 75.37%)\n",
      "\n",
      "Epoch 3/10\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6221, Train Acc: 79.47%\n",
      "Val Loss: 1.0038, Val Acc: 79.80%\n",
      "LR: 0.000794\n",
      "‚úÖ New best model saved! (Val Acc: 79.80%)\n",
      "\n",
      "Epoch 4/10\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6173, Train Acc: 79.47%\n",
      "Val Loss: 0.5418, Val Acc: 82.27%\n",
      "LR: 0.000655\n",
      "‚úÖ New best model saved! (Val Acc: 82.27%)\n",
      "\n",
      "Epoch 5/10\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3435, Train Acc: 88.89%\n",
      "Val Loss: 0.5969, Val Acc: 84.24%\n",
      "LR: 0.000500\n",
      "‚úÖ New best model saved! (Val Acc: 84.24%)\n",
      "\n",
      "Epoch 6/10\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2822, Train Acc: 90.58%\n",
      "Val Loss: 0.5405, Val Acc: 85.22%\n",
      "LR: 0.000345\n",
      "‚úÖ New best model saved! (Val Acc: 85.22%)\n",
      "\n",
      "Epoch 7/10\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1749, Train Acc: 93.00%\n",
      "Val Loss: 0.5056, Val Acc: 87.19%\n",
      "LR: 0.000206\n",
      "‚úÖ New best model saved! (Val Acc: 87.19%)\n",
      "\n",
      "Epoch 8/10\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1079, Train Acc: 95.89%\n",
      "Val Loss: 0.4418, Val Acc: 88.67%\n",
      "LR: 0.000095\n",
      "‚úÖ New best model saved! (Val Acc: 88.67%)\n",
      "\n",
      "Epoch 9/10\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0932, Train Acc: 97.10%\n",
      "Val Loss: 0.4403, Val Acc: 87.68%\n",
      "LR: 0.000024\n",
      "\n",
      "Epoch 10/10\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0680, Train Acc: 98.31%\n",
      "Val Loss: 0.4558, Val Acc: 87.68%\n",
      "LR: 0.000000\n",
      "\n",
      "üèÅ TRAINING COMPLETE!\n",
      "=========================\n",
      "‚è±Ô∏è Training time: 1070.18s\n",
      "üéØ Best validation accuracy: 88.67%\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
