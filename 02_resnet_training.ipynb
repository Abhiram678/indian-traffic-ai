{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet with attention training\n\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-24T11:55:27.690012Z"
    }
   },
   "source": [
    "# ResNet with attention training\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ResNet50 + Attention Training Pipeline\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Create directories\n",
    "models_dir = Path(\"../../models/saved_models\")\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "results_plots_dir = Path(\"../../results/plots\")\n",
    "results_plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Created directories:\")\n",
    "print(f\"   {models_dir}\")\n",
    "print(f\"   {results_plots_dir}\")\n",
    "\n",
    "# Load configuration\n",
    "processed_root = Path(\"../../data/processed\")\n",
    "results_dir = Path(\"../../results\")\n",
    "\n",
    "with open(processed_root / \"training_config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "class_weights = torch.load(processed_root / \"fast_class_weights.pt\", map_location='cpu')\n",
    "\n",
    "print(f\"\\nTraining Configuration:\")\n",
    "print(f\"   Classes: {config['num_classes']}\")\n",
    "print(f\"   Class names: {config['class_names']}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Load dataset code (reusing from EfficientNet)\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "# Load simplified mappings\n",
    "with open(results_dir / \"simplified_class_mapping.json\", \"r\") as f:\n",
    "    class_mapping = json.load(f)\n",
    "\n",
    "with open(results_dir / \"detailed_to_simplified_mapping.json\", \"r\") as f:\n",
    "    detailed_to_simplified = json.load(f)\n",
    "\n",
    "class FastVehicleDataset:\n",
    "    \"\"\"Lightweight dataset for training\"\"\"\n",
    "    \n",
    "    def __init__(self, split_name, max_files=100, transform=None):\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data(split_name, max_files)\n",
    "        print(f\"{split_name} dataset: {len(self.data)} samples\")\n",
    "    \n",
    "    def _load_data(self, split_name, max_files):\n",
    "        data_root = Path(\"../../data/raw\")\n",
    "        annos_dir = data_root / split_name / \"annos\"\n",
    "        images_dir = data_root / split_name / \"images\"\n",
    "        \n",
    "        xml_files = list(annos_dir.glob(\"*.xml\"))[:max_files]\n",
    "        data_samples = []\n",
    "        \n",
    "        for xml_file in xml_files:\n",
    "            try:\n",
    "                tree = ET.parse(xml_file)\n",
    "                root = tree.getroot()\n",
    "                \n",
    "                img_id = xml_file.stem\n",
    "                img_path = images_dir / f\"{img_id}.jpg\"\n",
    "                \n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "                \n",
    "                for obj in root.findall('object'):\n",
    "                    try:\n",
    "                        name_elem = obj.find('name')\n",
    "                        if name_elem is None:\n",
    "                            continue\n",
    "                        \n",
    "                        detailed_class = name_elem.text.strip().lower()\n",
    "                        simplified_class = detailed_to_simplified.get(detailed_class, 'unknown')\n",
    "                        \n",
    "                        if simplified_class == 'unknown':\n",
    "                            continue\n",
    "                        \n",
    "                        bbox_elem = obj.find('bndbox')\n",
    "                        if bbox_elem is None:\n",
    "                            continue\n",
    "                        \n",
    "                        xmin = max(0, int(float(bbox_elem.find('xmin').text)))\n",
    "                        ymin = max(0, int(float(bbox_elem.find('ymin').text)))\n",
    "                        xmax = int(float(bbox_elem.find('xmax').text))\n",
    "                        ymax = int(float(bbox_elem.find('ymax').text))\n",
    "                        \n",
    "                        if xmax > xmin + 30 and ymax > ymin + 30:\n",
    "                            data_samples.append({\n",
    "                                'img_path': str(img_path),\n",
    "                                'bbox': [xmin, ymin, xmax, ymax],\n",
    "                                'class_idx': class_mapping[simplified_class],\n",
    "                                'class_name': simplified_class\n",
    "                            })\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return data_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            item = self.data[idx]\n",
    "            image = Image.open(item['img_path']).convert('RGB')\n",
    "            bbox = item['bbox']\n",
    "            cropped = image.crop(bbox)\n",
    "            \n",
    "            if self.transform:\n",
    "                cropped = self.transform(cropped)\n",
    "            \n",
    "            return cropped, item['class_idx']\n",
    "        except:\n",
    "            dummy_img = torch.zeros(3, 224, 224)\n",
    "            return dummy_img, 0\n",
    "\n",
    "# CBAM Attention Module\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ca = ChannelAttention(in_planes, ratio)\n",
    "        self.sa = SpatialAttention(kernel_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x * self.ca(x)\n",
    "        x = x * self.sa(x)\n",
    "        return x\n",
    "\n",
    "# ResNet50 with CBAM Attention\n",
    "class ResNetWithAttention(nn.Module):\n",
    "    def __init__(self, num_classes=6, pretrained=True):\n",
    "        super(ResNetWithAttention, self).__init__()\n",
    "        \n",
    "        # Load pretrained ResNet50\n",
    "        self.backbone = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        # Remove final layers\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        \n",
    "        # Add CBAM attention\n",
    "        self.attention = CBAM(2048)  # ResNet50 final feature size\n",
    "        \n",
    "        # Global average pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Custom classifier with attention\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended_features = self.attention(features)\n",
    "        \n",
    "        # Global pooling\n",
    "        pooled = self.global_pool(attended_features)\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "        \n",
    "        # Classification\n",
    "        output = self.classifier(pooled)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Enhanced transforms for ResNet\n",
    "def get_resnet_transforms(stage='train'):\n",
    "    if stage == 'train':\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomCrop((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# Create datasets\n",
    "print(f\"\\nCREATING RESNET TRAINING DATASETS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "train_dataset = FastVehicleDataset('train', max_files=100, transform=get_resnet_transforms('train'))\n",
    "val_dataset = FastVehicleDataset('val', max_files=50, transform=get_resnet_transforms('val'))\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True, num_workers=0)  # Smaller batch for ResNet\n",
    "val_loader = DataLoader(val_dataset, batch_size=6, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Data loaders ready:\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Create ResNet model\n",
    "print(f\"\\nCREATING RESNET50 + ATTENTION MODEL\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "model = ResNetWithAttention(num_classes=config['num_classes'])\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"ResNet50 + Attention model created\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Trainable: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Focal Loss for better handling of class imbalance\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2, weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Setup training\n",
    "print(f\"\\nTRAINING SETUP\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Focal loss with class weights\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2, weight=class_weights.to(device))\n",
    "\n",
    "# Optimizer with different learning rates for backbone and classifier\n",
    "backbone_params = []\n",
    "classifier_params = []\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name or 'attention' in name:\n",
    "        classifier_params.append(param)\n",
    "    else:\n",
    "        backbone_params.append(param)\n",
    "\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': backbone_params, 'lr': 0.0001},  # Lower LR for pretrained backbone\n",
    "    {'params': classifier_params, 'lr': 0.001}  # Higher LR for new layers\n",
    "], weight_decay=0.01)\n",
    "\n",
    "# Step learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "print(f\"Training setup complete:\")\n",
    "print(f\"   Loss: FocalLoss (alpha=0.25, gamma=2) with class weights\")\n",
    "print(f\"   Optimizer: AdamW with different LRs\")\n",
    "print(f\"   Scheduler: StepLR (step=5, gamma=0.5)\")\n",
    "\n",
    "# Training and validation functions\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with tqdm(train_loader, desc=\"Training\", leave=False) as pbar:\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.3f}',\n",
    "                'Acc': f'{100.*correct/total:.1f}%'\n",
    "            })\n",
    "    \n",
    "    return running_loss / len(train_loader), 100. * correct / total\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / len(val_loader), 100. * correct / total\n",
    "\n",
    "# Training loop\n",
    "print(f\"\\nSTARTING RESNET TRAINING\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "num_epochs = 12  # More epochs for ResNet\n",
    "best_acc = 0.0\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        model_save_path = models_dir / \"resnet_attention_best.pth\"\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"New best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nRESNET TRAINING COMPLETE!\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Training time: {training_time:.2f}s\")\n",
    "print(f\"Best validation accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Loss', color='blue', linewidth=2)\n",
    "plt.plot(val_losses, label='Val Loss', color='red', linewidth=2)\n",
    "plt.title('ResNet + Attention: Loss Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_accs, label='Train Acc', color='blue', linewidth=2)\n",
    "plt.plot(val_accs, label='Val Acc', color='red', linewidth=2)\n",
    "plt.title('ResNet + Attention: Accuracy Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Compare with EfficientNet if results exist\n",
    "plt.subplot(1, 3, 3)\n",
    "try:\n",
    "    with open(results_dir / \"efficientnet_training_results.json\", \"r\") as f:\n",
    "        efficientnet_results = json.load(f)\n",
    "    \n",
    "    plt.bar(['EfficientNet-B3', 'ResNet50+Attention'], \n",
    "            [efficientnet_results['best_val_accuracy'], best_acc],\n",
    "            color=['skyblue', 'lightcoral'])\n",
    "    plt.title('Model Comparison')\n",
    "    plt.ylabel('Best Validation Accuracy (%)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for i, v in enumerate([efficientnet_results['best_val_accuracy'], best_acc]):\n",
    "        plt.text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "        \n",
    "except:\n",
    "    plt.text(0.5, 0.5, 'EfficientNet results\\nnot found', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_plots_dir / \"resnet_attention_training_curves.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save training results\n",
    "training_results = {\n",
    "    'model': 'ResNet50+Attention',\n",
    "    'best_val_accuracy': float(best_acc),\n",
    "    'final_train_accuracy': float(train_accs[-1]),\n",
    "    'final_val_accuracy': float(val_accs[-1]),\n",
    "    'training_time': float(training_time),\n",
    "    'epochs': num_epochs,\n",
    "    'train_losses': train_losses,\n",
    "    'train_accs': train_accs,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accs': val_accs,\n",
    "    'parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'model_size_mb': sum(p.numel() * 4 for p in model.parameters()) / (1024 * 1024),\n",
    "    'architecture_features': {\n",
    "        'attention_mechanism': 'CBAM (Channel + Spatial)',\n",
    "        'loss_function': 'FocalLoss',\n",
    "        'learning_rate_schedule': 'Different LRs for backbone/classifier',\n",
    "        'regularization': 'Dropout + BatchNorm + Gradient Clipping'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(results_dir / \"resnet_attention_training_results.json\", \"w\") as f:\n",
    "    json.dump(training_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nRESULTS SAVED:\")\n",
    "print(f\"   Model weights: {model_save_path}\")\n",
    "print(f\"   Training curves: {results_plots_dir}/resnet_attention_training_curves.png\")\n",
    "print(f\"   Results: {results_dir}/resnet_attention_training_results.json\")\n",
    "\n",
    "print(f\"\\nFINAL RESULTS:\")\n",
    "print(f\"   Best Val Accuracy: {best_acc:.2f}%\")\n",
    "print(f\"   Final Train Accuracy: {train_accs[-1]:.2f}%\")\n",
    "print(f\"   Training Time: {training_time:.1f}s\")\n",
    "print(f\"   Model Size: {training_results['model_size_mb']:.1f}MB\")\n",
    "\n",
    "print(f\"\\nRESNET + ATTENTION TRAINING COMPLETE!\")\n",
    "print(\"Next: Train YOLO model for object detection\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 + Attention Training Pipeline\n",
      "=============================================\n",
      "Device: cpu\n",
      "Created directories:\n",
      "   ..\\..\\models\\saved_models\n",
      "   ..\\..\\results\\plots\n",
      "\n",
      "Training Configuration:\n",
      "   Classes: 6\n",
      "   Class names: ['auto_rickshaw', 'bus', 'car', 'motorcycle', 'scooter', 'truck']\n",
      "   Device: cpu\n",
      "\n",
      "CREATING RESNET TRAINING DATASETS\n",
      "========================================\n",
      "train dataset: 414 samples\n",
      "val dataset: 203 samples\n",
      "Data loaders ready:\n",
      "   Train batches: 69\n",
      "   Val batches: 34\n",
      "\n",
      "CREATING RESNET50 + ATTENTION MODEL\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\abhir/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:30<00:00, 3.40MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 + Attention model created\n",
      "   Parameters: 25,215,912\n",
      "   Trainable: 25,215,912\n",
      "\n",
      "TRAINING SETUP\n",
      "====================\n",
      "Training setup complete:\n",
      "   Loss: FocalLoss (alpha=0.25, gamma=2) with class weights\n",
      "   Optimizer: AdamW with different LRs\n",
      "   Scheduler: StepLR (step=5, gamma=0.5)\n",
      "\n",
      "STARTING RESNET TRAINING\n",
      "==============================\n",
      "\n",
      "Epoch 1/12\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2734, Train Acc: 39.86%\n",
      "Val Loss: 0.1929, Val Acc: 59.11%\n",
      "LR: 0.000100\n",
      "New best model saved! (Val Acc: 59.11%)\n",
      "\n",
      "Epoch 2/12\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2082, Train Acc: 50.72%\n",
      "Val Loss: 0.1766, Val Acc: 57.64%\n",
      "LR: 0.000100\n",
      "\n",
      "Epoch 3/12\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2104, Train Acc: 53.38%\n",
      "Val Loss: 0.1797, Val Acc: 43.35%\n",
      "LR: 0.000100\n",
      "\n",
      "Epoch 4/12\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 12/69 [00:23<01:50,  1.95s/it, Loss=0.128, Acc=54.2%]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
